totalPage$id <- NULL
totalPage$from_id <- NULL
totalPage$from_name <- NULL
totalPage$link <- NULL
totalPage$messagetext <- totalPage$message
totalPage$time <- substr(totalPage$created_time, start=12, stop=13) #抓出發文時間
str(totalPage)
test <- as.matrix(totalPage$message,nrow=nrow(totalPage) )
library(ggplot2)
ggplot(totalpage_test,aes(x=type,y=likes_count))+geom_boxplot()+theme_bw()
dotchart(as.numeric(totalPage$time))
View(test)
View(test)
totalPage$id <- NULL
totalPage$from_id <- NULL
totalPage$from_name <- NULL
totalPage$link <- NULL
totalPage$messagetext <- totalPage$message
totalPage$time <- substr(totalPage$created_time, start=12, stop=13) #抓出發文時間
str(totalPage)
test <- NULL
lapply(1:length(nrow(totalpage)),function(x){totalpage[[x]] %>% mutate(approach=x)}) %>%
do.call("rbind",.)
lapply(1:length(nrow(totalPage)),function(x){totalPage[[x]] %>% mutate(approach=x)}) %>%
do.call("rbind",.)
#install.packages("jiebaR")
library(jiebaR)
cutter <- worker()
new_user_word(cutter,'並不是',"n")
new_user_word(cutter,'有一天',"n")
new_user_word(cutter,'點個頭',"v")
new_user_word(cutter,'不一定',"n")
new_user_word(cutter,'那一年',"n")
new_user_word(cutter,'花了',"v")
new_user_word(cutter,'不喜歡',"v")
new_user_word(cutter,'一開始',"n")
readLines("stop.txt")
cutter = worker(stop_word ="stop.txt")
cutter$bylines<- F
library(dplyr)
lapply(1:length(nrow(totalPage)),function(x){totalPage[[x]] %>% mutate(approach=x)}) %>%
do.call("rbind",.)
d.cut <- cutter[totalPage$message]
data.table::rbindlist(d.cut,idcol=T)
install.packages("data.table")
library(data.table)
data.table::rbindlist(d.cut,idcol=T)
d.cut <-  (sort(table(cutter[totalPage$message]),decreasing = T)
data.table::rbindlist(d.cut,idcol=T)
d.cut <-  (sort(table(cutter[totalPage$message]),decreasing = T))
library(data.table)
data.table::rbindlist(d.cut,idcol=T)
str(d.cut)
?rbindlist
data.table::rbindlist(d.cut)
d.cut <- as.table(d.cut)
data.table::rbindlist(d.cut)
#install.packages("jiebaR")
library(jiebaR)
cutter <- worker()
new_user_word(cutter,'並不是',"n")
new_user_word(cutter,'有一天',"n")
new_user_word(cutter,'點個頭',"v")
new_user_word(cutter,'不一定',"n")
new_user_word(cutter,'那一年',"n")
new_user_word(cutter,'花了',"v")
new_user_word(cutter,'不喜歡',"v")
new_user_word(cutter,'一開始',"n")
readLines("stop.txt")
cutter = worker(stop_word ="stop.txt")
cutter$bylines<- F
d.cut <-  (sort(table(cutter[totalPage$message]),decreasing = T))
d.cut <- as.table(d.cut)
#install.packages("data.table")
library(data.table)
data.table::rbindlist(d.cut)
View(totalPage)
View(totalPage)
token<- "EAACEdEose0cBAHRbuRrlFE0Gw0DggNM0nIkKdNfvhSLag4blj99s51oTPqfGOiZCGMhxhHaUe7kpAoNq0VFCWdHLGnS82ifOP3hHhOjil7FZCZBLpnt2VZCZBRqoDc4fqVYhe3lHDNdSKc9r34ebWzCPegsZCg6SM3HblSIZATQOjICoud8ZAsPuBZAYi4WZCVK28ZD" #access token
#install.packages("Rfacebook")  #初次使用須先安裝
library(Rfacebook)
lastDate<-Sys.Date()
DateVector<-seq(as.Date("2017-05-01"),lastDate,by="1 days")  #目前以4月至今的貼文來做初步的分析
DateVectorStr<-as.character(DateVector)
totalPage<-NULL
for(i in 1:(length(DateVectorStr)-1)){
tempPage<-getPage("petesonline", token,
since = DateVectorStr[i],
until = DateVectorStr[i+1])
totalPage<-rbind(totalPage,tempPage)
}
token<- "EAACEdEose0cBAH1oZBvmBM4SKHerD7jI3tGpsZBrmXjZAp8S7P4qtth1KsFFjla4T6K3Y6ZBbQMwxRr1O2opHCyFhUDQYOZCDF6LbmU4BGAQKPo4RAZBxizMIT6vjzK0a2TzsGx4fdVoUctjZB3flAiGDk9V1bd07PcagFhpIn2ViROYmL1pJXXKwPsHpjPBvQZD" #access token
#install.packages("Rfacebook")  #初次使用須先安裝
library(Rfacebook)
lastDate<-Sys.Date()
DateVector<-seq(as.Date("2017-05-01"),lastDate,by="1 days")  #目前以4月至今的貼文來做初步的分析
DateVectorStr<-as.character(DateVector)
totalPage<-NULL
for(i in 1:(length(DateVectorStr)-1)){
tempPage<-getPage("petesonline", token,
since = DateVectorStr[i],
until = DateVectorStr[i+1])
totalPage<-rbind(totalPage,tempPage)
}
nrow(totalPage) #得知有幾筆資料
totalPage$id <- NULL
totalPage$from_id <- NULL
totalPage$from_name <- NULL
totalPage$link <- NULL
totalPage$time <- substr(totalPage$created_time, start=12, stop=13) #抓出發文時間
str(totalPage)
test <- NULL
text.c <- cutter[totalPage$message]
token<- "EAACEdEose0cBAH1oZBvmBM4SKHerD7jI3tGpsZBrmXjZAp8S7P4qtth1KsFFjla4T6K3Y6ZBbQMwxRr1O2opHCyFhUDQYOZCDF6LbmU4BGAQKPo4RAZBxizMIT6vjzK0a2TzsGx4fdVoUctjZB3flAiGDk9V1bd07PcagFhpIn2ViROYmL1pJXXKwPsHpjPBvQZD" #access token
#install.packages("Rfacebook")  #初次使用須先安裝
library(Rfacebook)
library(jiebaR)
cutter <- worker()
new_user_word(cutter,'並不是',"n")
new_user_word(cutter,'有一天',"n")
new_user_word(cutter,'點個頭',"v")
new_user_word(cutter,'不一定',"n")
new_user_word(cutter,'那一年',"n")
new_user_word(cutter,'花了',"v")
new_user_word(cutter,'不喜歡',"v")
new_user_word(cutter,'一開始',"n")
readLines("stop.txt")
cutter = worker(stop_word ="stop.txt")
lastDate<-Sys.Date()
DateVector<-seq(as.Date("2017-05-01"),lastDate,by="1 days")  #目前以4月至今的貼文來做初步的分析
DateVectorStr<-as.character(DateVector)
totalPage<-NULL
for(i in 1:(length(DateVectorStr)-1)){
tempPage<-getPage("petesonline", token,
since = DateVectorStr[i],
until = DateVectorStr[i+1])
totalPage<-rbind(totalPage,tempPage)
text.c <- cutter[totalPage$message]
}
nrow(totalPage) #得知有幾筆資料
View(totalPage)
View(totalPage)
View(totalPage)
View(totalPage)
totalPage$id <- NULL
totalPage$from_id <- NULL
totalPage$from_name <- NULL
totalPage$link <- NULL
totalPage$time <- substr(totalPage$created_time, start=12, stop=13) #抓出發文時間
str(totalPage)
test <- NULL
a <- (sort(table(cutter[totalPage$message]),decreasing = T))
#install.packages("wordcloud")  安裝wordcloud 套件
library(wordcloud)
#install.packages("RColorBrewer") # color palettes
#install.packages("SnowballC")
library("SnowballC")
library(RColorBrewer)
#install.packages("tm")
library(tm)
library(tmcn)
library(reshape2)
a <- tm_map(a, removePunctuation)
a <- as.data.frame(sort(table(cutter[totalPage$message]),decreasing = T))
#install.packages("wordcloud")  安裝wordcloud 套件
library(wordcloud)
#install.packages("RColorBrewer") # color palettes
#install.packages("SnowballC")
library("SnowballC")
library(RColorBrewer)
#install.packages("tm")
library(tm)
library(tmcn)
library(reshape2)
a <- tm_map(a, removePunctuation)
a <- as.matrix.data.frame(sort(table(cutter[totalPage$message]),decreasing = T))
a <- as.matrix.(sort(table(cutter[totalPage$message]),decreasing = T))
a <- as.matrix(sort(table(cutter[totalPage$message]),decreasing = T))
#install.packages("wordcloud")  安裝wordcloud 套件
library(wordcloud)
#install.packages("RColorBrewer") # color palettes
#install.packages("SnowballC")
library("SnowballC")
library(RColorBrewer)
#install.packages("tm")
library(tm)
library(tmcn)
library(reshape2)
a <- tm_map(a, removePunctuation)
a<- tm_map(a)
a <- (sort(table(cutter[totalPage$message]),decreasing = T)
View(a)
View(a)
str(a)
a[[1]]
a[[]V1]
a[[V1]]
a[V1]
dimnames(a)
m.t <- as.matrix(a[[1]],nrow= nrow(totalPage))
View(m.t)
View(m.t)
m.t <- as.matrix(a[1],nrow= nrow(totalPage))
View(m.t)
View(m.t)
m.t <- as.matrix(a[1],nrow= nrow(totalPage))
View(m.t)
View(m.t)
View(m.t)
View(m.t)
m.t <- NULL
text.c <- NULL
v <- (sort(table(cutter[totalPage$message]),decreasing = T))
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(d$word, d$freq, min.freq = 10, random.order = F, ordered.colors = F )
wordcloud(d$word, d$freq, min.freq = 10, random.order = T, ordered.colors = T )
wordcloud(d$word, d$freq, min.freq = 10, random.order = F, ordered.colors = F,
colors = rainbow)
head(d, 10)
token<- "EAACEdEose0cBAAYhUTEc6Sh1IbikTb4JrhI2PL124f7ZAZCLhfvHUXFsXnQBUUkEflkxAfiK0uL7ZBZAsKWTvPqf1a6rO7aZAoZBMRahdLvcSmDj34cT3y4ZA2KZBFyelqvkN3OuubCcQNCbXntDolBGVMx0HDwImKzLWp1YMKJrAsMrrgNh0MlBksLs2LSL5UkZD" #access token
#install.packages("Rfacebook")  #初次使用須先安裝
library(Rfacebook)
lastDate<-Sys.Date()
DateVector<-seq(as.Date("2017-05-01"),lastDate,by="1 days")  #目前以5月至今的貼文來做初步的分析
DateVectorStr<-as.character(DateVector)
totalPage<-NULL
for(i in 1:(length(DateVectorStr)-1)){
tempPage<-getPage("petesonline", token,
since = DateVectorStr[i],
until = DateVectorStr[i+1])
totalPage<-rbind(totalPage,tempPage)
}
nrow(totalPage) #得知有幾筆資料
totalPage$id <- NULL
totalPage$from_id <- NULL
totalPage$from_name <- NULL
totalPage$link <- NULL
totalPage$time <- substr(totalPage$created_time, start=12, stop=13) #抓出發文時間
str(totalPage)
test <- NULL
#install.packages("jiebaR")
library(jiebaR)
cutter <- worker()
new_user_word(cutter,'並不是',"n")
new_user_word(cutter,'有一天',"n")
new_user_word(cutter,'點個頭',"v")
new_user_word(cutter,'不一定',"n")
new_user_word(cutter,'那一年',"n")
new_user_word(cutter,'花了',"v")
new_user_word(cutter,'不喜歡',"v")
new_user_word(cutter,'一開始',"n")
readLines("stop.txt")
cutter = worker(stop_word ="stop.txt")
cutter[totalPage$message]
#install.packages("ggplot2")
library(ggplot2)
ggplot(totalpage_test,aes(x=type,y=likes_count))+geom_boxplot()+theme_bw()
mean(totalPage$likes_count)  #平均每篇貼文的讚數
mean(totalPage$comments_count) #下方留言平均數
mean(totalPage$shares_count) # 分享次篇貼文的次數
range(totalPage$shares_count)
library(dplyr)
totalpage_test <- NULL
totalpage_test <- totalPage
totalpage_test %>%
group_by(type) %>%
summarize(num_likes = mean(likes_count),
num_comment = mean(comments_count),
num_share  = mean(shares_count)) %>%
arrange(desc(num_likes))
#sort(table(cutter[totalPage$message]),decreasing = T)
sort(table(cutter[test]),decreasing = T)
ggplot(totalpage_test,aes(x=type,y=likes_count))+geom_boxplot()+theme_bw()
dotchart(as.numeric(totalPage$time))
sort(table(cutter[totalPage]),decreasing = T)
sort(table(cutter[totalPage$message]),decreasing = T)
EAACEdEose0cBAAYhUTEc6Sh1IbikTb4JrhI2PL124f7ZAZCLhfvHUXFsXnQBUUkEflkxAfiK0uL7ZBZAsKWTvPqf1a6rO7aZAoZBMRahdLvcSmDj34cT3y4ZA2KZBFyelqvkN3OuubCcQNCbXntDolBGVMx0HDwImKzLWp1YMKJrAsMrrgNh0MlBksLs2LSL5UkZD
?wordcloud
#install.packages("wordcloud")  安裝wordcloud 套件
library(wordcloud)
#install.packages("RColorBrewer") # color palettes
#install.packages("SnowballC")
library("SnowballC")
library(RColorBrewer)
#install.packages("tm")
library(tm)
library(tmcn)
library(reshape2)
?wordcloud
a <- (sort(table(cutter[totalPage$message]),decreasing = T))
#install.packages("wordcloud")  安裝wordcloud 套件
library(wordcloud)
#install.packages("RColorBrewer") # color palettes
#install.packages("SnowballC")
library("SnowballC")
library(RColorBrewer)
#install.packages("tm")
library(tm)
library(tmcn)
library(reshape2)
a <- tm_map(a, removePunctuation)
a <- (sort(table(cutter[totalPage$message]),decreasing = T))
#install.packages("wordcloud")  安裝wordcloud 套件
library(wordcloud)
#install.packages("RColorBrewer") # color palettes
#install.packages("SnowballC")
library("SnowballC")
library(RColorBrewer)
#install.packages("tm")
library(tm)
library(tmcn)
library(reshape2)
#a <- tm_map(a, removePunctuation)
#a <- tm_map(a, removeNumbers)
#a <- tm_map(a, function(word) {
#    gsub("[A-Za-z0-9]", "", word)})
#m <- as.matrix(dtm)
v <- (sort(table(cutter[totalPage$message]),decreasing = T))
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud(d$word,d$freq,scale=c(2,.Inf),min.freq=5,max.words=Inf,
random.order=F, random.color=FALSE, rot.per=.1,
colors="Red",ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
View(d)
View(d)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(d$word,d$freq,scale=c(2,.Inf),min.freq=5,max.words=Inf,
random.order=T, random.color=FALSE, rot.per=.1,
colors="Red",ordered.colors=FALSE,use.r.layout=FALSE)
View(d)
View(d)
a <- NULL
a <- tm_map(v, removePunctuation)
v <- (sort(table(cutter[totalPage$message]),decreasing = T))
d <- data.frame(word = names(v),freq=v)
head(d, 10)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
findFreqTerms((sort(table(cutter[totalPage$message]), lowfreq = 4))
findFreqTerms(sort(table(cutter[totalPage$message]), lowfreq = 4)
findFreqTerms(sort(table(cutter[totalPage$message])), lowfreq = 4)
View(totalpage_test)
View(totalpage_test)
page.cut <- cutter[totalPage$message]
page.cut <- NULL
m1 <- as.matrix(totalPage)
v <- sort(rowSums(totalPage), decreasing = TRUE)
m1 <- as.matrix(totalPage)
v <- sort(rowSums(m1), decreasing = TRUE)
test <- data.frame(totalPage$message)
View(test)
View(test)
write.table(test,file="test.txt",sep=",",row.names = F,col.names = T)
write.table(test,file="test.txt",sep=",",row.names = F,col.names = T,fileEncoding= UTF-8)
write.table(test,file="test.txt",sep=",",row.names = F,col.names = T,fileEncoding= "UTF-8")
docu <- read_delim("D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt", file =               delim="\t")
docu <- read_delim("D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt", file =               delim="\n")
docu <- read_delim(file = "D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt",               delim="\n")
library(readr)
docu <- read_delim(file = "D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt",               delim="\n")
View(docu)
View(docu)
str(docu)
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docu, toSpace, "/")
library(readr)
docu <- read_delim(file = "D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt",               delim="\n")
dtm <- TermDocumentMatrix(docu)
write.table(test,file="test.txt",sep="",row.names = F,col.names = T,fileEncoding= "UTF-8")
library(readr)
docu <- read_delim(file = "D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt",               delim="\t")
str(docu)
?write.table()
write.table(test,file="test.txt",sep="",row.names = F,col.names = T,fileEncoding= "UTF-8",eol = "\r\n")
m1 <- as.matrix(docu)
v <- sort(rowSums(m1), decreasing = TRUE)
str(m1)
View(test)
View(test)
write.table(test,file="test.txt",row.names = F,col.names = T,fileEncoding= "UTF-8",eol = "\r\n")
write.table(test,file="test.txt",row.names = F,col.names = T,fileEncoding= "UTF-8",eol = "\r\n")
write.table(test,file="test.txt",row.names = F,col.names = T,fileEncoding= "UTF-8",eol = "\r\n")
text <- readLines("D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt")
docs <- Corpus(VectorSource(text))
inspect(docs)
write.table(test,file="test.txt",row.names = F,col.names = T,fileEncoding= "UTF-8",eol = "\r\n")
text <- readLines("D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt")
docs <- Corpus(VectorSource(text))
docu <-NULL
m1 <- as.matrix(docs)
View(d)
View(d)
#install.packages("wordcloud")  安裝wordcloud 套件
#install.packages("tm")
#install.packages("RColorBrewer") # color palettes
#install.packages("SnowballC")
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(tm)
library(tmcn)
library(reshape2)
library(jiebaR)
cutter2 <- worker()
new_user_word(cutter2,'並不是',"n")
new_user_word(cutter2,'有一天',"n")
new_user_word(cutter2,'點個頭',"v")
new_user_word(cutter2,'不一定',"n")
new_user_word(cutter2,'那一年',"n")
new_user_word(cutter2,'花了',"v")
new_user_word(cutter2,'不喜歡',"v")
new_user_word(cutter2,'一開始',"n")
readLines("stop.txt")
cutter = worker(stop_word ="stop.txt")
cutter[docs]
cutter[docs[[1]]]
docs[[1]]
cutter[docs[[]]]
cutter[docs[[,]]]
docs <- NULL
cutter[text]
?jiebaR
#install.packages("wordcloud")  安裝wordcloud 套件
#install.packages("tm")
#install.packages("RColorBrewer") # color palettes
#install.packages("SnowballC")
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(tm)
library(tmcn)
library(reshape2)
library(jiebaR)
cutter2 <- worker()
new_user_word(cutter2,'並不是',"n")
new_user_word(cutter2,'有一天',"n")
new_user_word(cutter2,'點個頭',"v")
new_user_word(cutter2,'不一定',"n")
new_user_word(cutter2,'那一年',"n")
new_user_word(cutter2,'花了',"v")
new_user_word(cutter2,'不喜歡',"v")
new_user_word(cutter2,'一開始',"n")
readLines("stop.txt")
cutter2 = worker(stop_word ="stop.txt")
cutter2[text]
dtm <- TermDocumentMatrix(docs)
write.table(test,file="test.txt",row.names = F,col.names = T,fileEncoding= "UTF-8",eol = "\r\n")
text <- readLines("D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt")
cutter2 = worker(stop_word ="stop.txt")
cutter2[text]
write.table(test,file="test.txt",row.names = F,col.names = T,eol = "\r\n")
text <- readLines("D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt")
cutter2 = worker(stop_word ="stop.txt")
cutter2[text]
docs <- cutter2[text]
cutter2[text]
docs<- NULL
v <- sort(cutter2[text],decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
v <- sort(table(cutter2[text]),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
d <- NULL
v <- sort(table(cutter2[text]),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
head(d, 10)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
?wordcloud()
wordcloud(words = d$word, freq = d$freq, scale = c(-Inf,.Inf), min.freq = 1,
max.words=200, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, scale = c(-Inf,.Inf), min.freq = 1,
max.words=Inf, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
View(d)
View(d)
v <- sort(table(cutter2[text]),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
View(d)
View(d)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq.Freq , scale = c(-Inf,.Inf), min.freq = 1,
max.words=Inf, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq.Freq ,  min.freq = 1,
max.words=Inf, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq.Freq ,  min.freq = 5,
max.words=10, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq.Freq ,  min.freq = 5,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = d$word, freq = d$freq.Freq ,  min.freq = 1,
max.words=100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
