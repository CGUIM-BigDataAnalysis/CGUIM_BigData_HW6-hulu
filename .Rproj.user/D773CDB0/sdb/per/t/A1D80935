{
    "collab_server" : "",
    "contents" : "---\ntitle: \"勵志書暢銷的背後，吸引讀者的詞彙為何？\"\noutput: github_document\n---\n\n\n###胡茹芳\n## 分析議題背景\n在這出版業人人喊苦,書店一間接著一間熄燈\n勵志書籍能和工具書並駕齊驅甚至高居排行榜不下\nPeter Su 是近年暢銷書的作家,在百花齊放的社群上有著高人氣\n出版的著作有著驚人的銷量\n在這出版業哀聲載道,一刷2000本能賣完是萬幸的寒冬\n還能賣出十萬本的書籍究竟是深藏著何種功夫\n在暢銷的背後，讀者追尋的是什麼？\n\n## 分析動機\nhttp://www.openbook.org.tw/article/20170225-249\n看到這篇文章覺得很有意思,也想到今年於台北國際書展\n勵志書籍的數量相當驚人,也放在各出版社的醒目位置\n走過去總會忍不住拿起來翻個幾頁或是將封面書腰上的文字快速閱讀過\n網路上對於此類的書籍評價正反兩極 (正方認為能夠帶來舒緩;反方持無病呻吟的看法)\nPeter Su 是先由FB貼文引起關注而逐漸擁有許多粉絲,才得以出書\n能夠在短短一個月55刷衝破10本的銷量\n不論內容是什麼,總是讓我很好奇\n\n## 使用資料\n打算抓暢銷作家PeterSu在FB的貼文\n時間是2017四月至今\n\n\n載入使用資料們\n```{r}\ntoken<- \"EAACEdEose0cBAAYhUTEc6Sh1IbikTb4JrhI2PL124f7ZAZCLhfvHUXFsXnQBUUkEflkxAfiK0uL7ZBZAsKWTvPqf1a6rO7aZAoZBMRahdLvcSmDj34cT3y4ZA2KZBFyelqvkN3OuubCcQNCbXntDolBGVMx0HDwImKzLWp1YMKJrAsMrrgNh0MlBksLs2LSL5UkZD\" #access token \n#install.packages(\"Rfacebook\")  #初次使用須先安裝\nlibrary(Rfacebook) \nlastDate<-Sys.Date()\nDateVector<-seq(as.Date(\"2017-05-01\"),lastDate,by=\"1 days\")  #目前以5月至今的貼文來做初步的分析\nDateVectorStr<-as.character(DateVector)\ntotalPage<-NULL\nfor(i in 1:(length(DateVectorStr)-1)){\n    tempPage<-getPage(\"petesonline\", token,\n                since = DateVectorStr[i],\n                until = DateVectorStr[i+1])\n    totalPage<-rbind(totalPage,tempPage)\n}\n nrow(totalPage) #得知有幾筆資料\n```\n\n## 資料處理與清洗\n將totalPage裡的id,from_id,from_name的欄位拿掉\n\n處理資料\n```{r}\ntotalPage$id <- NULL\ntotalPage$from_id <- NULL\ntotalPage$from_name <- NULL\ntotalPage$link <- NULL\ntotalPage$time <- substr(totalPage$created_time, start=12, stop=13) #抓出發文時間\nstr(totalPage)\ntest <- data.frame(totalPage$message)\n```\n\n## 探索式資料分析\n透過結巴來進行中文斷詞,看什麼詞彙是作者頻繁使用\n來得知內容多半會涉及哪方面\n利用group_by 來分析發文附上影片,圖片,連結之間是否會有差距懸殊的互動\n(在此假設互動就是有在貼文進行留言,按讚,或分享)\n\n```{r}\n#install.packages(\"jiebaR\")\nlibrary(jiebaR)\ncutter <- worker()\nnew_user_word(cutter,'並不是',\"n\")\nnew_user_word(cutter,'有一天',\"n\")\nnew_user_word(cutter,'點個頭',\"v\")\nnew_user_word(cutter,'不一定',\"n\")\nnew_user_word(cutter,'那一年',\"n\")\nnew_user_word(cutter,'花了',\"v\")\nnew_user_word(cutter,'不喜歡',\"v\")\nnew_user_word(cutter,'一開始',\"n\")\nreadLines(\"stop.txt\")\ncutter = worker(stop_word =\"stop.txt\")\ncutter[totalPage$message]\n\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\nggplot(totalpage_test,aes(x=type,y=likes_count))+geom_boxplot()+theme_bw() \ndotchart(as.numeric(totalPage$time))\n```\n\n```{r}\nwrite.table(test,file=\"test.txt\",row.names = F,col.names = T,eol = \"\\r\\n\")\ntext <- readLines(\"D:/DocumentsAndSettings/Documents/GitHub/CGUIM_BigData_HW6-hulu/test.txt\")\n\n\n```\n\n\n\n```{r}\n#install.packages(\"wordcloud\")  安裝wordcloud 套件\n#install.packages(\"tm\") \n#install.packages(\"RColorBrewer\") # color palettes\n#install.packages(\"SnowballC\")\nlibrary(wordcloud)\nlibrary(SnowballC)\nlibrary(RColorBrewer)\nlibrary(tm)\nlibrary(tmcn)\nlibrary(reshape2)\nlibrary(jiebaR)\ncutter2 <- worker()\nnew_user_word(cutter2,'並不是',\"n\")\nnew_user_word(cutter2,'有一天',\"n\")\nnew_user_word(cutter2,'點個頭',\"v\")\nnew_user_word(cutter2,'不一定',\"n\")\nnew_user_word(cutter2,'那一年',\"n\")\nnew_user_word(cutter2,'花了',\"v\")\nnew_user_word(cutter2,'不喜歡',\"v\")\nnew_user_word(cutter2,'一開始',\"n\")\nreadLines(\"stop.txt\")\ncutter2 = worker(stop_word =\"stop.txt\")\ncutter2[text]\n\n\ndtm <- TermDocumentMatrix(docs)\nm <- as.matrix(dtm)\n\n\nv <- sort(table(cutter2[text]),decreasing=TRUE)\nd <- data.frame(word = names(v),freq=v)\nhead(d, 10)\nset.seed(1234)\nwordcloud(words = d$word, freq = d$freq.Freq ,  min.freq = 1,\n          max.words=100, random.order=FALSE, rot.per=0.35, \n          colors=brewer.pal(8, \"Dark2\"))\n```\n\n```{r}\nmean(totalPage$likes_count)  #平均每篇貼文的讚數\nmean(totalPage$comments_count) #下方留言平均數\nmean(totalPage$shares_count) # 分享次篇貼文的次數\nrange(totalPage$shares_count)\n\nlibrary(dplyr)\ntotalpage_test <- NULL\ntotalpage_test <- totalPage\ntotalpage_test %>% \n  group_by(type) %>%   \n          summarize(num_likes = mean(likes_count),\n                    num_comment = mean(comments_count),\n                    num_share  = mean(shares_count)) %>%\n                  arrange(desc(num_likes))\nsort(table(cutter[totalPage$message]),decreasing = T)\n```\n## 期末專題分析規劃\n期末打算分析PeterSu 去年的FB貼文\n(他目前為天下雜誌駐站作家．Brand名牌誌專欄作家．Cheers駐站作家)\n再利用wordcloud去產生讓人一目瞭然的文字雲\nhttps://www.r-bloggers.com/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know/\nhttp://rstudio-pubs-static.s3.amazonaws.com/12422_b2b48bb2da7942acaca5ace45bd8c60c.html\n藉由上面兩篇文章,想試著練習Wordcloud\n並統計其貼文被轉貼的數量 \n一個月內書可以再刷到55刷 在這紙本書產業蕭條的時代\n什麼內容能如同一道佳餚抓住讀者的胃口\n並藉由發文時間了解是否作者有自己的一個固定與臉友的互動時光\n將新增自訂詞彙 自訂停止詞 讓分析可以更精準\n\n\n前情提要:你是否在書店書展中看到滿滿的勵志書籍在大平台上而目光被拉走\n在資訊量爆炸,容易使人焦躁不安的世代\n網路上出現了許多勵志短文\n\n期末專題要做XXOOO交叉分析\n",
    "created" : 1496842106076.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "872111260",
    "id" : "A1D80935",
    "lastKnownWriteTime" : 1497069425,
    "last_content_update" : -2147483648,
    "path" : "~/GitHub/CGUIM_BigData_HW6-hulu/README.Rmd",
    "project_path" : "README.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}